{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "word2vec_embeddings = pd.read_parquet('/pio/scratch/1/recommender_systems/processed/word2vec/amazon-clothes/5-core/item_item_embeddings.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                   0         1         2         3         4         5  \\\nindex                                                                    \nB00SU52460 -0.078735  0.031405  0.057666  0.090253  0.055262  0.004454   \nB00STBRT9M -0.068344  0.023045  0.061581  0.102385  0.074816  0.014265   \nB00SU5244M -0.075733  0.023512  0.059046  0.085086  0.056547  0.011237   \nB00WRGPHY4 -0.072343  0.012610  0.053967  0.094973  0.055312  0.006420   \nB00V6K7P0G -0.063687  0.027942  0.054714  0.093803  0.067540 -0.003258   \n...              ...       ...       ...       ...       ...       ...   \nB00EVWCKGK -0.009977  0.114948  0.026678  0.038227  0.067839 -0.124184   \nB019T96K7Y -0.012741  0.075559  0.031464  0.040446  0.022506 -0.156780   \nB00Q1O4BQI  0.004234  0.083226  0.014779  0.080230  0.023185 -0.120007   \nB0087DXEW8 -0.021674  0.073881  0.014785  0.030988  0.050229 -0.122902   \nB00HWQNFZM -0.008637  0.090850  0.018336  0.060426  0.054338 -0.133097   \n\n                   6         7         8         9  ...        90        91  \\\nindex                                               ...                       \nB00SU52460  0.068138  0.140214 -0.160675 -0.001144  ... -0.028228 -0.120070   \nB00STBRT9M  0.068359  0.128264 -0.166192 -0.011253  ... -0.026627 -0.105604   \nB00SU5244M  0.074188  0.118348 -0.144083 -0.033818  ... -0.030343 -0.120872   \nB00WRGPHY4  0.070272  0.129301 -0.158411 -0.018321  ... -0.036316 -0.110264   \nB00V6K7P0G  0.080216  0.139799 -0.150908 -0.019819  ... -0.017860 -0.111070   \n...              ...       ...       ...       ...  ...       ...       ...   \nB00EVWCKGK  0.146636  0.247998 -0.134573 -0.112451  ...  0.162464  0.008654   \nB019T96K7Y  0.026824  0.282930 -0.168034 -0.121483  ...  0.157675 -0.015005   \nB00Q1O4BQI  0.155374  0.236055 -0.146596 -0.120077  ...  0.122532 -0.004304   \nB0087DXEW8  0.105506  0.210099 -0.158622 -0.153365  ...  0.111065  0.056616   \nB00HWQNFZM  0.132497  0.218774 -0.143452 -0.114978  ...  0.125210 -0.012576   \n\n                  92        93        94        95        96        97  \\\nindex                                                                    \nB00SU52460 -0.141606 -0.066599  0.224218 -0.110949  0.103306 -0.067999   \nB00STBRT9M -0.149194 -0.069288  0.214240 -0.120073  0.093751 -0.074356   \nB00SU5244M -0.117350 -0.074961  0.228749 -0.112058  0.121153 -0.060023   \nB00WRGPHY4 -0.128595 -0.070194  0.231627 -0.117734  0.111027 -0.080635   \nB00V6K7P0G -0.136759 -0.065150  0.219500 -0.126879  0.110436 -0.077520   \n...              ...       ...       ...       ...       ...       ...   \nB00EVWCKGK -0.039211 -0.000062  0.206180 -0.016075  0.065916 -0.071067   \nB019T96K7Y -0.056732 -0.096309  0.183719 -0.005037  0.041290 -0.012217   \nB00Q1O4BQI -0.033180 -0.010378  0.234828 -0.030729  0.090226 -0.060556   \nB0087DXEW8 -0.071574 -0.025645  0.198892 -0.000518  0.115308 -0.071822   \nB00HWQNFZM -0.060314 -0.030525  0.220940 -0.044699  0.098995 -0.093853   \n\n                  98        99  \nindex                           \nB00SU52460  0.113143  0.087766  \nB00STBRT9M  0.106935  0.087801  \nB00SU5244M  0.108016  0.094660  \nB00WRGPHY4  0.110474  0.094160  \nB00V6K7P0G  0.109715  0.083321  \n...              ...       ...  \nB00EVWCKGK  0.019635  0.025326  \nB019T96K7Y  0.058867  0.020988  \nB00Q1O4BQI  0.000169  0.009621  \nB0087DXEW8  0.028719  0.019632  \nB00HWQNFZM  0.019382  0.000701  \n\n[242400 rows x 100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>B00SU52460</th>\n      <td>-0.078735</td>\n      <td>0.031405</td>\n      <td>0.057666</td>\n      <td>0.090253</td>\n      <td>0.055262</td>\n      <td>0.004454</td>\n      <td>0.068138</td>\n      <td>0.140214</td>\n      <td>-0.160675</td>\n      <td>-0.001144</td>\n      <td>...</td>\n      <td>-0.028228</td>\n      <td>-0.120070</td>\n      <td>-0.141606</td>\n      <td>-0.066599</td>\n      <td>0.224218</td>\n      <td>-0.110949</td>\n      <td>0.103306</td>\n      <td>-0.067999</td>\n      <td>0.113143</td>\n      <td>0.087766</td>\n    </tr>\n    <tr>\n      <th>B00STBRT9M</th>\n      <td>-0.068344</td>\n      <td>0.023045</td>\n      <td>0.061581</td>\n      <td>0.102385</td>\n      <td>0.074816</td>\n      <td>0.014265</td>\n      <td>0.068359</td>\n      <td>0.128264</td>\n      <td>-0.166192</td>\n      <td>-0.011253</td>\n      <td>...</td>\n      <td>-0.026627</td>\n      <td>-0.105604</td>\n      <td>-0.149194</td>\n      <td>-0.069288</td>\n      <td>0.214240</td>\n      <td>-0.120073</td>\n      <td>0.093751</td>\n      <td>-0.074356</td>\n      <td>0.106935</td>\n      <td>0.087801</td>\n    </tr>\n    <tr>\n      <th>B00SU5244M</th>\n      <td>-0.075733</td>\n      <td>0.023512</td>\n      <td>0.059046</td>\n      <td>0.085086</td>\n      <td>0.056547</td>\n      <td>0.011237</td>\n      <td>0.074188</td>\n      <td>0.118348</td>\n      <td>-0.144083</td>\n      <td>-0.033818</td>\n      <td>...</td>\n      <td>-0.030343</td>\n      <td>-0.120872</td>\n      <td>-0.117350</td>\n      <td>-0.074961</td>\n      <td>0.228749</td>\n      <td>-0.112058</td>\n      <td>0.121153</td>\n      <td>-0.060023</td>\n      <td>0.108016</td>\n      <td>0.094660</td>\n    </tr>\n    <tr>\n      <th>B00WRGPHY4</th>\n      <td>-0.072343</td>\n      <td>0.012610</td>\n      <td>0.053967</td>\n      <td>0.094973</td>\n      <td>0.055312</td>\n      <td>0.006420</td>\n      <td>0.070272</td>\n      <td>0.129301</td>\n      <td>-0.158411</td>\n      <td>-0.018321</td>\n      <td>...</td>\n      <td>-0.036316</td>\n      <td>-0.110264</td>\n      <td>-0.128595</td>\n      <td>-0.070194</td>\n      <td>0.231627</td>\n      <td>-0.117734</td>\n      <td>0.111027</td>\n      <td>-0.080635</td>\n      <td>0.110474</td>\n      <td>0.094160</td>\n    </tr>\n    <tr>\n      <th>B00V6K7P0G</th>\n      <td>-0.063687</td>\n      <td>0.027942</td>\n      <td>0.054714</td>\n      <td>0.093803</td>\n      <td>0.067540</td>\n      <td>-0.003258</td>\n      <td>0.080216</td>\n      <td>0.139799</td>\n      <td>-0.150908</td>\n      <td>-0.019819</td>\n      <td>...</td>\n      <td>-0.017860</td>\n      <td>-0.111070</td>\n      <td>-0.136759</td>\n      <td>-0.065150</td>\n      <td>0.219500</td>\n      <td>-0.126879</td>\n      <td>0.110436</td>\n      <td>-0.077520</td>\n      <td>0.109715</td>\n      <td>0.083321</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>B00EVWCKGK</th>\n      <td>-0.009977</td>\n      <td>0.114948</td>\n      <td>0.026678</td>\n      <td>0.038227</td>\n      <td>0.067839</td>\n      <td>-0.124184</td>\n      <td>0.146636</td>\n      <td>0.247998</td>\n      <td>-0.134573</td>\n      <td>-0.112451</td>\n      <td>...</td>\n      <td>0.162464</td>\n      <td>0.008654</td>\n      <td>-0.039211</td>\n      <td>-0.000062</td>\n      <td>0.206180</td>\n      <td>-0.016075</td>\n      <td>0.065916</td>\n      <td>-0.071067</td>\n      <td>0.019635</td>\n      <td>0.025326</td>\n    </tr>\n    <tr>\n      <th>B019T96K7Y</th>\n      <td>-0.012741</td>\n      <td>0.075559</td>\n      <td>0.031464</td>\n      <td>0.040446</td>\n      <td>0.022506</td>\n      <td>-0.156780</td>\n      <td>0.026824</td>\n      <td>0.282930</td>\n      <td>-0.168034</td>\n      <td>-0.121483</td>\n      <td>...</td>\n      <td>0.157675</td>\n      <td>-0.015005</td>\n      <td>-0.056732</td>\n      <td>-0.096309</td>\n      <td>0.183719</td>\n      <td>-0.005037</td>\n      <td>0.041290</td>\n      <td>-0.012217</td>\n      <td>0.058867</td>\n      <td>0.020988</td>\n    </tr>\n    <tr>\n      <th>B00Q1O4BQI</th>\n      <td>0.004234</td>\n      <td>0.083226</td>\n      <td>0.014779</td>\n      <td>0.080230</td>\n      <td>0.023185</td>\n      <td>-0.120007</td>\n      <td>0.155374</td>\n      <td>0.236055</td>\n      <td>-0.146596</td>\n      <td>-0.120077</td>\n      <td>...</td>\n      <td>0.122532</td>\n      <td>-0.004304</td>\n      <td>-0.033180</td>\n      <td>-0.010378</td>\n      <td>0.234828</td>\n      <td>-0.030729</td>\n      <td>0.090226</td>\n      <td>-0.060556</td>\n      <td>0.000169</td>\n      <td>0.009621</td>\n    </tr>\n    <tr>\n      <th>B0087DXEW8</th>\n      <td>-0.021674</td>\n      <td>0.073881</td>\n      <td>0.014785</td>\n      <td>0.030988</td>\n      <td>0.050229</td>\n      <td>-0.122902</td>\n      <td>0.105506</td>\n      <td>0.210099</td>\n      <td>-0.158622</td>\n      <td>-0.153365</td>\n      <td>...</td>\n      <td>0.111065</td>\n      <td>0.056616</td>\n      <td>-0.071574</td>\n      <td>-0.025645</td>\n      <td>0.198892</td>\n      <td>-0.000518</td>\n      <td>0.115308</td>\n      <td>-0.071822</td>\n      <td>0.028719</td>\n      <td>0.019632</td>\n    </tr>\n    <tr>\n      <th>B00HWQNFZM</th>\n      <td>-0.008637</td>\n      <td>0.090850</td>\n      <td>0.018336</td>\n      <td>0.060426</td>\n      <td>0.054338</td>\n      <td>-0.133097</td>\n      <td>0.132497</td>\n      <td>0.218774</td>\n      <td>-0.143452</td>\n      <td>-0.114978</td>\n      <td>...</td>\n      <td>0.125210</td>\n      <td>-0.012576</td>\n      <td>-0.060314</td>\n      <td>-0.030525</td>\n      <td>0.220940</td>\n      <td>-0.044699</td>\n      <td>0.098995</td>\n      <td>-0.093853</td>\n      <td>0.019382</td>\n      <td>0.000701</td>\n    </tr>\n  </tbody>\n</table>\n<p>242400 rows × 100 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "PCA(n_components=2)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(word2vec_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def analyse_ABX(path, items_embeddings, pca):\n",
    "    abx_tests = pd.read_json(path, lines=True)\n",
    "    A = np.zeros((10000, 100))\n",
    "    B = np.zeros((10000, 100))\n",
    "    X = np.zeros((10000, 100))\n",
    "\n",
    "    for i, test in abx_tests.iterrows():\n",
    "        A[i, :] = items_embeddings.loc[test[\"A\"]]\n",
    "        B[i, :] = items_embeddings.loc[test[\"B\"]]\n",
    "        X[i, :] = items_embeddings.loc[test[\"X\"]]\n",
    "\n",
    "    dist_A = ((A - X)**2).sum(axis=1)\n",
    "    dist_B = ((B - X)**2).sum(axis=1)\n",
    "\n",
    "    cos_dist_A = np.zeros(10000)\n",
    "    cos_dist_B = np.zeros(10000)\n",
    "\n",
    "    for i in range(10000):\n",
    "        cos_dist_A[i] = spatial.distance.cosine(A[i, :], X[i, :])\n",
    "        cos_dist_B[i] = spatial.distance.cosine(B[i, :], X[i, :])\n",
    "\n",
    "    if pca is not None:\n",
    "        pca_A = pca.transform(A)\n",
    "        pca_B = pca.transform(B)\n",
    "        pca_X = pca.transform(X)\n",
    "\n",
    "        dist_pca_A = ((pca_A - pca_X)**2).sum(axis=1)\n",
    "        dist_pca_B  = ((pca_B - pca_X)**2).sum(axis=1)\n",
    "\n",
    "        cos_dist_pca_A = np.zeros(10000)\n",
    "        cos_dist_pca_B = np.zeros(10000)\n",
    "\n",
    "        for i in range(10000):\n",
    "            cos_dist_pca_A[i] = spatial.distance.cosine(pca_A[i, :], pca_X[i, :])\n",
    "            cos_dist_pca_B[i] = spatial.distance.cosine(pca_B[i, :], pca_X[i, :])\n",
    "\n",
    "    return [(dist_A < dist_B).mean(), (dist_pca_A < dist_pca_B).mean(), ((dist_A < dist_B) == (dist_pca_A < dist_pca_B)).mean()], [(cos_dist_A < cos_dist_B).mean(), (cos_dist_pca_A < cos_dist_pca_B).mean(), ((cos_dist_A < cos_dist_B) == (cos_dist_pca_A < cos_dist_pca_B)).mean()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "([0.6349, 0.6428, 0.8697], [0.6349, 0.6496, 0.7293])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyse_ABX('/pio/scratch/1/recommender_systems/interim/ABX_tests/5_core.json', word2vec_embeddings, pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}